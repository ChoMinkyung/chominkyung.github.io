---
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
toc_sticky: true

layout:
title: "[Deep Learning] 1st chapter : Linear Models"
excerpt:
date: 2022-04-14
last_modified_at: 2022-04-14
categories:
  - Deep
tags:
  - [인공지능, 머신러닝, 딥러닝]

use_math: true
comments: true
share: false
---

---

<div style="text-align: right"> Lecture 2</div>

## 1st chapter : Linear Models

### \* 딥러닝 
: 입력(데이터)에 대한 출력을 내보낼 때 다수의 레이어를 사용하는 것 → 여러 레이어들이 합성 함수처럼 작동됨

1. 딥러닝의 장점 : <br>
  - 많은 라이브러리(ex)TensorFlow, PyTorch, Caffe2, …)<br>
  - ML 알고리즘보다 간단한 알고리즘<br>
  - 대규모 데이터셋으로 확장 가능<br>
  - 컴퓨터 비전과 자동 음성 인식분야에서의 높은 성능<br>
2.  딥러닝의 한계<br>
  - 많은 레이블 데이터의 필요<br>
  - uncertainty하거나 knowledge한 데이터를 다루기 어려움<br>
  - 결과에 대한 해석의 어려움<br>
  - hyperparameter optimization의 어려움<br>
  - 훼손되거나 noise가 생긴 데이터에 취약<br>
  
### \* Linear Models for Regression 
  1. Linear Model<br>
    - weighted sum of features로 regularization을 적용하지 않은 모델 <br>
    - 실제 값과 예측값의 차이(오류의 제곱 값, RSS:Residual Sum of Squares)를 최소화하도록 회귀계수를 최적화<br>

  2. Regression : 레이블 데이터 D가 주어질 때 x→y로 mapping되는 f(x)를 찾는 것<br>
    - linear regression : feature가 하나일 때 어떤 직선을 학습하는 알고리즘<br>
    - polynomial regression : 다항식을 사용하여 feature와 y 사이의 관계를 나타낸 선형 회귀<br>
      - linear vs. non-linear : 회귀에서 선형/비선형 회귀를 구분하는 기준은 독립변수 x가 아닌 회귀 계수(weight)에 대한 선형 여부이므로 polynomial regression도 linear regression에 포함 <br>
  3. Ordinary Least Squares : RSS를 최소화하는 weight 벡터를 구하는 것<br>
  4. Regularization : 모델이 data에 과적합해지는 것을 막기 위함<br>
    - L(w) = Data Loss function + λR(w)(regularization term)<br>
    - L1 regularization : weight 절대값의 합<br>
    - L2 regularization : 제곱한 weight의 합<br>
  5. Ridge Regression : 선형회귀에 L2 regularization을 추가한 회귀 모델<br>
  6. Logistic Regression : sigmoid 함수가 예측하는 0과 1 사이의 값을 확률로 간주하여 그 확률에 따라 데이터를 두 class로 분류하는 binary classification <br>
  7. Multiclass Extension : softmax regression<br>
    - 3개 이상의 class로 분류하는 다중 클래스 분류 문제 해결에 사용<br>
    - softmax function : 선택해야하는 class의 총 개수를 k라고 할 때, k차원의 벡터를 입력받아 각 클래스에 대한 확률 추정<br>
  8. Cross Entropy Error<br>
    - p(실제값), q(예측값)의 차이를 계산하는 것<br>
    - 예측값이 실제값과 가까울수록, 다른 예측값과 차이가 클수록  cross entropy error는 감소<br>