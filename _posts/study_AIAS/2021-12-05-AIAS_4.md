---
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
toc_sticky: true

layout:
title: "[인공지능 응용 시스템] Lecture2 - 4. Dimensionality Reduction/Expansion, 5. Linear Classifier"
excerpt:
date: 2021-12-05
last_modified_at: 2021-12-05
categories:
  - AIAS
tags:
  - [인공지능, 머신러닝, 딥러닝]

use_math: true
comments: true
share: false
---

---

<div style="text-align: right"> Lecture 2</div>

## 4. Dimensionality Reduction/Expansion

<br>
- 일반적으로 차원이 늘어나면 비례해서 data도 늘어나야 합니다. data는 늘어나지 않고 차원만 늘어난다면 처음에는 성능향상을 보이지만 일정 수준을 지나면 성능이 떨어지는 “Curse of dimensionality(차원의 저주)” 현상이 생깁니다. 이 현상을 막기 위해 머신러닝에서는 차원이 너무 커지지 않도록 제한하는 것이 필요합니다. 따라서 차원의 수도 하나의 hyperparameter로  적절한 값을 찾는 것이 중요합니다.
<br>

![CurseOfDimension](https://user-images.githubusercontent.com/58170545/144749669-1ba35ebf-58f1-4c4c-9920-45dda17ac01f.png){: .align-center width="50%" height="50%"}

### \* Dimensionality Reduction
<br>
(1) PCA : 데이터 분포를 가장 잘 표현하는 축으로 좌표축 이동
<br>
(2) LDA : 데이터 간 class를 잘 구분하는 축으로 좌표축 이동
<br>
(3) t-sne : visualization
<br>

### \* Dimensionality Expansion

데이터의 분포가 현재 차원에서 제대로 구분이 안될 때, class로 구분하기 어려울 때 차원을 확장합니다. 예를 들어 두 데이터 분포가 서로 섞여 있어 2차원에서는 구분이 불가할 때 z축을 추가하여 3차원으로 늘린다면 두 데이터가 명확히 구분될 수 있을 것입니다. 즉, 두 데이터가 현재 차원에서 classifier 할 수 없을 때 kernel을 이용하여 차원을 확장합니다. 차원을 확장하면 현재 차원에서는 볼 수 없는 class간 구분선이 생기게 됩니다. 보통 차원 확장은 잘 하지않고 차원 축소를 주로 하지만 경우에 따라 kernel을 사용하여 차원을 확장하여 줍니다.<br>

## 5. Linear Classifier

n개의 classes가 있는 문제에서 Input x를 구분할 때 함수 f(x,W)를 사용하여 각 class의 score를 구합니다.<br>

* f(x,W) = Wx+b<br>
* x: input, W : weights(학습에 따라 변하는 hyper parameter), b : bias<br>

W를 update하여 정답(class)의 score를 올려(Loss Function 감소시키는 것) input x의 class를 구하는 것입니다.  Input x를 함수 f에 통과(feed-forward)시켜 나온 값과 정답을 Loss Function으로 비교하여 W를 update하고 이 과정을 반복합니다. 이렇게 반복하는 것을 Optimization(최적화)라고 합니다. 이상적으로는 Loss function이 0이 되거나 0에 가까워지고 정답 class의 score가 처음에 주어진 정답셋의 score와 가까워 질 것입니다. 이때, input과 상관없는 bias를 더해주어야 학습이 잘 이루집니다.<br>

![optimization](https://user-images.githubusercontent.com/58170545/144751602-c857dd73-ba56-4d7e-9d8c-6bf73b8f65ab.png){: .align-center width="50%" height="50%"}

<br>
* W의 행렬 = score의 크기 * input x의 크기, score의 크기의 bias를 추가하는 parameter 필요<br>
<br>
보통 linear classifier는 선을 그어서 class를 구분지어야 하는데 선으로 구분되지 않는 문제들도 있습니다. 그래서 linear classifier만 사용하지 어렵기 때문에 linear classifier로 여러 층을 쌓아 Deep Neural network를 만들어 사용합니다. 하지만 linear한 함수들을 많이 곱하여도 linear한 성질이 유지되기 때문에 nonlinearity한 activation function을 더해주어야 합니다. 초기에는 activation function으로 sigmoid를 주로 사용했지만 최근에는 tanh도 많이 사용합니다. <br>