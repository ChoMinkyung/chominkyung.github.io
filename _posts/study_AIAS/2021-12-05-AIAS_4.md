---
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
toc_sticky: true

layout:
title: "[인공지능 응용 시스템] Lecture2 - 4. Dimensionality Reduction/Expansion"
excerpt:
date: 2021-12-05
last_modified_at: 2021-12-05
categories:
  - AIAS
tags:
  - [인공지능, 머신러닝, 딥러닝]

use_math: true
comments: true
share: false
---

---

<div style="text-align: right"> Lecture 2</div>

## Dimensionality Reduction/Expansion

<br>
- 일반적으로 차원이 늘어나면 비례해서 data도 늘어나야 합니다. data는 늘어나지 않고 차원만 늘어난다면 처음에는 성능향상을 보이지만 일정 수준을 지나면 성능이 떨어지는 “Curse of dimensionality(차원의 저주)” 현상이 생깁니다. 이 현상을 막기 위해 머신러닝에서는 차원이 너무 커지지 않도록 제한하는 것이 필요합니다. 따라서 차원의 수도 하나의 hyperparameter로  적절한 값을 찾는 것이 중요합니다.
<br>

![CurseOfDimension](https://user-images.githubusercontent.com/58170545/144749669-1ba35ebf-58f1-4c4c-9920-45dda17ac01f.png){: .align-center width="50%" height="50%"}

### \* Dimensionality Reduction
<br>
(1) PCA : 데이터 분포를 가장 잘 표현하는 축으로 좌표축 이동
<br>
(2) LDA : 데이터 간 class를 잘 구분하는 축으로 좌표축 이동
<br>
(3) t-sne : visualization
<br>

### \* Dimensionality Expansion

데이터의 분포가 현재 차원에서 제대로 구분이 안될 때, class로 구분하기 어려울 때 차원을 확장합니다. 예를 들어 두 데이터 분포가 서로 섞여 있어 2차원에서는 구분이 불가할 때 z축을 추가하여 3차원으로 늘린다면 두 데이터가 명확히 구분될 수 있을 것입니다. 즉, 두 데이터가 현재 차원에서 classifier 할 수 없을 때 kernel을 이용하여 차원을 확장합니다. 차원을 확장하면 현재 차원에서는 볼 수 없는 class간 구분선이 생기게 됩니다. 보통 차원 확장은 잘 하지않고 차원 축소를 주로 하지만 경우에 따라 kernel을 사용하여 차원을 확장하여 줍니다.<br>

### \* Distance Metrix